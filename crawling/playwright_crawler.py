"""
ë„¤ì´ë²„ ë¦¬ë·° í¬ë¡¤ëŸ¬ (Playwright ì‚¬ìš©)
ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤/ì§€ë„ì—ì„œ ë¦¬ë·°ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ê°€ê²Œ ì´ë¦„ì´ë‚˜ ì£¼ì†Œë§Œ ì…ë ¥í•˜ë©´ ìë™ìœ¼ë¡œ ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ë¥¼ ì°¾ì•„ì„œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""
import asyncio
from playwright.async_api import async_playwright
from typing import List, Dict, Tuple, Optional
import re

def clean_review_text(text: str) -> str:
    """
    ë„¤ì´ë²„ ë¦¬ë·° í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜

    ì›¹í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ë¦¬ë·°ì—ëŠ” "ë¦¬ë·° 56", "ì‚¬ì§„ 164", "íŒ”ë¡œì›Œ 3" ê°™ì€
    UI ë©”íƒ€ë°ì´í„°ê°€ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ ë¦¬ë·° ë‚´ìš©ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        text: í¬ë¡¤ë§í•œ ì›ë³¸ ë¦¬ë·° í…ìŠ¤íŠ¸ (UI ìš”ì†Œì™€ ì„ì—¬ìˆìŒ)

    Returns:
        ì •ì œëœ ìˆœìˆ˜ ë¦¬ë·° ë³¸ë¬¸

    Example:
        ì…ë ¥: "ë¦¬ë·° 56\\nì‚¬ì§„ 164\\në§›ìˆì–´ìš”\\níŒ”ë¡œì›Œ 3"
        ì¶œë ¥: "ë§›ìˆì–´ìš”"
    """
    # ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì¤„ë¡œ ë¶„ë¦¬
    lines = text.split('\n')

    # ì œê±°í•  UI ìš”ì†Œ íŒ¨í„´ë“¤ (ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©)
    # r'íŒ¨í„´'ì€ ì •ê·œí‘œí˜„ì‹(regex)ì„ ì˜ë¯¸í•˜ë©°, íŠ¹ì • í˜•íƒœì˜ í…ìŠ¤íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤
    noise_patterns = [
        r'^ë¦¬ë·°\s+\d+',                          # "ë¦¬ë·° 56" í˜•íƒœ ì œê±°
        r'^ì‚¬ì§„\s+\d+',                          # "ì‚¬ì§„ 164" í˜•íƒœ ì œê±°
        r'íŒ”ë¡œì›Œ?\s+\d+',                        # "íŒ”ë¡œì›Œ 3", "íŒ”ë¡œìš° 3" ì œê±°
        r'^\d+\s*íŒ”ë¡œìš°',                        # "3 íŒ”ë¡œìš°" ì œê±°
        r'ë°©ë¬¸ì¼\s+\d+\.\d+\.',                  # "ë°©ë¬¸ì¼ 9.14." ì œê±°
        r'\d{4}ë…„\s+\d{1,2}ì›”\s+\d{1,2}ì¼',     # "2025ë…„ 9ì›” 14ì¼" ì œê±°
        r'[ì¼ì›”í™”ìˆ˜ëª©ê¸ˆí† ]ìš”ì¼',                 # ìš”ì¼ ì •ë³´ ì œê±°
        r'\d+ë²ˆì§¸\s+ë°©ë¬¸',                       # "1ë²ˆì§¸ ë°©ë¬¸" ì œê±°
        r'ì¸ì¦\s+ìˆ˜ë‹¨',                          # "ì¸ì¦ ìˆ˜ë‹¨" ì œê±°
        r'ì˜ìˆ˜ì¦|ê²°ì œë‚´ì—­',                      # "ì˜ìˆ˜ì¦", "ê²°ì œë‚´ì—­" ì œê±°
        r'ë”\s*ë³´ê¸°',                            # "ë”ë³´ê¸°" ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        r'í¼ì³ë³´ê¸°',                             # "í¼ì³ë³´ê¸°" ë²„íŠ¼ í…ìŠ¤íŠ¸ ì œê±°
        r'ë°˜ì‘\s+ë‚¨ê¸°ê¸°',                        # "ë°˜ì‘ ë‚¨ê¸°ê¸°" ì œê±°
        r'ê°œì˜\s+ë¦¬ë·°ê°€\s+ë”\s+ìˆìŠµë‹ˆë‹¤',        # ë¦¬ë·° ê°œìˆ˜ ì•ˆë‚´ ë¬¸êµ¬ ì œê±°
        r'^\s*[+â€»]\d+\s*$',                     # "+4", "â€»3" ê°™ì€ ì‹¬ë³¼ ì œê±°
        r'ì˜ˆì•½\s+ì—†ì´\s+ì´ìš©',                   # ì˜ˆì•½ ì •ë³´ ì œê±°
        r'ëŒ€ê¸°\s+ì‹œê°„\s+ë°”ë¡œ\s+ì…ì¥',            # ëŒ€ê¸°ì‹œê°„ ì •ë³´ ì œê±°
        r'[ì €ì ]ì‹¬ì—?\s+ë°©ë¬¸',                   # "ì €ë…ì— ë°©ë¬¸", "ì ì‹¬ì— ë°©ë¬¸" ì œê±°
        r'ì¼ìƒ|ì¹œëª©|ë°ì´íŠ¸|ë‚˜ë“¤ì´',              # ë°©ë¬¸ ëª©ì  íƒœê·¸ ì œê±°
        r'í˜¼ì|ì—°ì¸ãƒ»ë°°ìš°ì|ì¹œêµ¬|ê°€ì¡±|ì•„ì´',     # ë™ë°˜ì íƒœê·¸ ì œê±°
        r'@\w+',                                 # ì¸ìŠ¤íƒ€ê·¸ë¨ íƒœê·¸(@username) ì œê±°
    ]

    cleaned_lines = []  # ì •ì œëœ í…ìŠ¤íŠ¸ ë¼ì¸ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    # ê° ì¤„ì„ í•˜ë‚˜ì”© ê²€ì‚¬
    for line in lines:
        line = line.strip()  # ì•ë’¤ ê³µë°± ì œê±°
        if not line:  # ë¹ˆ ì¤„ì€ ê±´ë„ˆë›°ê¸°
            continue

        # ì´ ì¤„ì´ ë…¸ì´ì¦ˆ(ë¶ˆí•„ìš”í•œ UI ìš”ì†Œ)ì¸ì§€ ì²´í¬
        is_noise = False
        for pattern in noise_patterns:
            if re.search(pattern, line):  # íŒ¨í„´ì´ ë°œê²¬ë˜ë©´
                is_noise = True
                break
        
        # ì§§ì€ UI í…ìŠ¤íŠ¸ í•„í„°ë§ (íŠ¹ì • í‚¤ì›Œë“œ ì œì™¸)
        short_keywords = ['ìŒì‹ì´ ë§›ìˆì–´ìš”', 'ë§¤ì¥ì´ ì²­ê²°í•´ìš”', 'ì¹œì ˆí•´ìš”', 'ê°€ì„±ë¹„ê°€ ì¢‹ì•„ìš”', 
                         'ì–‘ì´ ë§ì•„ìš”', 'ë§¤ì¥ì´ ë„“ì–´ìš”', 'í˜¼ë°¥í•˜ê¸° ì¢‹ì•„ìš”', 'íŠ¹ë³„í•œ ë©”ë‰´ê°€ ìˆì–´ìš”',
                         'ì¬ë£Œê°€ ì‹ ì„ í•´ìš”', 'ì¸í…Œë¦¬ì–´ê°€ ë©‹ì ¸ìš”', 'ë‹¨ì²´ëª¨ì„ í•˜ê¸° ì¢‹ì•„ìš”',
                         'ë·°ê°€ ì¢‹ì•„ìš”', 'íŠ¹ë³„í•œ ë‚  ê°€ê¸° ì¢‹ì•„ìš”', 'í™”ì¥ì‹¤ì´ ê¹¨ë—í•´ìš”',
                         'ì°¨ë¶„í•œ ë¶„ìœ„ê¸°ì˜ˆìš”', 'ëŒ€í™”í•˜ê¸° ì¢‹ì•„ìš”', 'ì•„ëŠ‘í•´ìš”', 'ì•„ì´ì™€ ê°€ê¸° ì¢‹ì•„ìš”',
                         'ë©”ë‰´ êµ¬ì„±ì´ ì•Œì°¨ìš”']
        
        # ë„¤ì´ë²„ ìë™ í‚¤ì›Œë“œëŠ” ê±´ë„ˆë›°ê¸° (ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°)
        if line.startswith('"') and any(keyword in line for keyword in short_keywords):
            continue
        
        # ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ë¼ì¸ ì œê±° (í‰ì , ë°©ë¬¸ íšŸìˆ˜ ë“±)
        if re.match(r'^\d+$', line):
            continue
        
        if not is_noise and len(line) > 3:  # ìµœì†Œ 4ê¸€ì ì´ìƒ
            cleaned_lines.append(line)
    
    # ë¦¬ë·° ë³¸ë¬¸ ì¬êµ¬ì„±
    review_text = ' '.join(cleaned_lines)
    
    # ì¶”ê°€ ì •ì œ: íŠ¹ìˆ˜ë¬¸ì ê³¼ë‹¤ ì œê±°
    review_text = re.sub(r'[+â€»~]{2,}', '', review_text)  # +++, ~~~~ ê°™ì€ ë°˜ë³µ ì œê±°
    review_text = re.sub(r'\s+', ' ', review_text)  # ë‹¤ì¤‘ ê³µë°± ì œê±°
    
    return review_text.strip()


async def search_place_and_get_url(query: str) -> Optional[Tuple[str, str]]:
    """
    ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê°€ê²Œë¥¼ ê²€ìƒ‰í•˜ì—¬ ìë™ìœ¼ë¡œ í”Œë ˆì´ìŠ¤ URLê³¼ ê°€ê²Œëª…ì„ ì°¾ìŠµë‹ˆë‹¤.

    ì‚¬ìš©ìê°€ "ë°”ëŒë‚œ ì–¼í° ìˆ˜ì œë¹„ ë²”ê³„ì " ë˜ëŠ” "ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123" ê°™ì€
    ê°€ê²Œ ì´ë¦„ì´ë‚˜ ì£¼ì†Œë¥¼ ì…ë ¥í•˜ë©´, ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰ì„ í†µí•´ ìë™ìœ¼ë¡œ
    í•´ë‹¹ ê°€ê²Œì˜ ë¦¬ë·° í˜ì´ì§€ URLì„ ì°¾ì•„ì¤ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰í•  ê°€ê²Œ ì´ë¦„ ë˜ëŠ” ì£¼ì†Œ (ì˜ˆ: "ë°”ëŒë‚œ ì–¼í° ìˆ˜ì œë¹„ ë²”ê³„ì ")

    Returns:
        (ë¦¬ë·° URL, ê°€ê²Œëª…) íŠœí”Œ, ì‹¤íŒ¨ ì‹œ None
        ì˜ˆ: ("https://m.place.naver.com/restaurant/31264425/review/visitor", "ë°”ëŒë‚œ ì–¼í° ìˆ˜ì œë¹„ ë²”ê³„ì ")
    """
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless=TrueëŠ” í™”ë©´ ì—†ì´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        browser = await p.chromium.launch(headless=True)

        # ëª¨ë°”ì¼ í™˜ê²½ìœ¼ë¡œ ì„¤ì • (ëª¨ë°”ì¼ í˜ì´ì§€ê°€ ë” ì•ˆì •ì )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
            viewport={"width": 375, "height": 812}
        )

        page = await context.new_page()

        try:
            # ë„¤ì´ë²„ ì§€ë„ ëª¨ë°”ì¼ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
            search_url = f"https://m.map.naver.com/search2/search.naver?query={query}"
            print(f"ğŸ” ë„¤ì´ë²„ ì§€ë„ì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")

            await page.goto(search_url, wait_until="networkidle", timeout=30000)
            await page.wait_for_timeout(2000)  # ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°

            # 1. ì´ë¯¸ í”Œë ˆì´ìŠ¤ ìƒì„¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²½ìš° ì²´í¬
            current_url = page.url
            if "m.place.naver.com" in current_url and ("/restaurant/" in current_url or "/place/" in current_url):
                print("âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ë°”ë¡œ ìƒì„¸ í˜ì´ì§€ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                place_href = current_url
                store_name = query 
            else:
                # 2. ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¥ì†Œë“¤ ì°¾ê¸° (ì¤‘ë³µ ì œê±° ë¡œì§ ì¶”ê°€)
                # ê²€ìƒ‰ ê²°ê³¼ ì•„ì´í…œ ì»¨í…Œì´ë„ˆë¥¼ ë¨¼ì € ì°¾ìŒ
                # ë³´í†µ li íƒœê·¸ ì•ˆì— _item_common_... í´ë˜ìŠ¤ ë“±ì„ ê°€ì§
                # í•˜ì§€ë§Œ ëª¨ë°”ì¼ ì›¹ êµ¬ì¡°ê°€ ë³µì¡í•˜ë¯€ë¡œ, a íƒœê·¸ë¥¼ ì°¾ë˜ ë¶€ëª¨ë¥¼ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ì œê±°
                
                raw_links = await page.locator('a[href*="/place/"], a[href*="/restaurant/"]').all()
                
                unique_places = []
                seen_ids = set()
                
                for link in raw_links:
                    href = await link.get_attribute('href')
                    # ID ì¶”ì¶œ
                    id_match = re.search(r'/(?:restaurant|place)/(\d+)', href)
                    if not id_match:
                        continue
                        
                    place_id = id_match.group(1)
                    if place_id in seen_ids:
                        continue
                        
                    # ìƒˆë¡œìš´ ì¥ì†Œ ë°œê²¬
                    seen_ids.add(place_id)
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¶€ëª¨ ìš”ì†Œì—ì„œ)
                    try:
                        # ë§í¬ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ (ì´ë¯¸ì§€ ë§í¬ ë“±)
                        # ë”°ë¼ì„œ ë¶€ëª¨ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, ë§í¬ ìì²´ì˜ í…ìŠ¤íŠ¸ë¥¼ í™•ì¸
                        text = await link.inner_text()
                        if not text.strip():
                            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¶€ëª¨ë‚˜ í˜•ì œ ìš”ì†Œì—ì„œ ì°¾ê¸° ì‹œë„
                            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë¶€ëª¨ í…ìŠ¤íŠ¸ ì‚¬ìš©
                            parent = link.locator("..")
                            text = await parent.inner_text()
                        
                        text = text.replace("\n", " ").strip()
                        # ë„ˆë¬´ ì§§ê±°ë‚˜ ì´ìƒí•œ í…ìŠ¤íŠ¸ í•„í„°ë§ (ì˜ˆ: "ì£¼ì†Œë³´ê¸°", "ê³µìœ " ë“±)
                        if len(text) < 2 or text in ["ì£¼ì†Œë³´ê¸°", "ê³µìœ ", "ì§€ë„ë³´ê¸°"]:
                            # ìƒìœ„ ë¶€ëª¨ì—ì„œ ë‹¤ì‹œ ì‹œë„
                            grandparent = link.locator("../..")
                            text = await grandparent.inner_text()
                            text = text.replace("\n", " ").strip()
                            
                        unique_places.append((link, text, href))
                    except:
                        continue
                        
                    if len(unique_places) >= 10: # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ìˆ˜ì§‘
                        break

                if not unique_places:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    await browser.close()
                    return None

                # ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ (ì •í™•ë„ í–¥ìƒ)
                filtered_places = []
                exact_matches = []
                
                # ì¿¼ë¦¬ ì •ê·œí™” (ê³µë°± ì œê±°)
                normalized_query = query.replace(" ", "")
                
                for link, text, href in unique_places:
                    # í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²Œ ì´ë¦„ë§Œ ì¶”ì¶œ ì‹œë„ (ì²« ë²ˆì§¸ ì¤„ or ê³µë°± ì „ê¹Œì§€)
                    # ì˜ˆ: "ì´ëª¨ë„¤ì •ìœ¡ì‹ë‹¹ ì •ìœ¡ì‹ë‹¹..." -> "ì´ëª¨ë„¤ì •ìœ¡ì‹ë‹¹"
                    # ì˜ˆ: "ì´ëª¨ë„¤ í•œì‹..." -> "ì´ëª¨ë„¤"
                    
                    # 1. ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²« ì¤„ í™•ì¸
                    first_line = text.split('\n')[0].strip()
                    
                    # 2. ì´ë¦„ ì •ê·œí™”
                    normalized_name = first_line.replace(" ", "")
                    
                    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° (ì´ëª¨ë„¤ == ì´ëª¨ë„¤)
                    if normalized_name == normalized_query:
                        exact_matches.append((link, text, href))
                        continue
                        
                    # ì¿¼ë¦¬ê°€ ì´ë¦„ì— í¬í•¨ë˜ëŠ” ê²½ìš° (ì´ëª¨ë„¤ê¹€ë°¥, ì´ëª¨ë„¤ì‹ë‹¹)
                    if normalized_query in normalized_name:
                        filtered_places.append((link, text, href))
                
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ë³´ì—¬ì¤Œ (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
                if exact_matches:
                    print(f"âœ¨ '{query}'ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œë¥¼ ìš°ì„  í‘œì‹œí•©ë‹ˆë‹¤.")
                    final_places = exact_matches
                else:
                    final_places = filtered_places

                if not final_places:
                    # í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš© (í˜¹ì‹œ ëª¨ë¥¼ ì˜¤ë¥˜ ë°©ì§€)
                    final_places = unique_places

                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ì‚¬ìš©ìì—ê²Œ ì„ íƒ ìš”ì²­
                if len(final_places) > 1:
                    print(f"\nğŸ¤” '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ {len(final_places)}ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print("-" * 50)
                    
                    for i, (link, text, href) in enumerate(final_places):
                        # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
                        display_text = text
                        if len(display_text) > 60:
                            display_text = display_text[:57] + "..."
                        print(f"[{i+1}] {display_text}")
                    
                    print("-" * 50)
                    
                    # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
                    try:
                        selection = input("ğŸ‘‰ ë¶„ì„í•  ê°€ê²Œ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (ê¸°ë³¸ê°’ 1): ").strip()
                        if not selection:
                            selected_idx = 0
                        else:
                            selected_idx = int(selection) - 1
                            if selected_idx < 0 or selected_idx >= len(final_places):
                                print("âš ï¸ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1ë²ˆì„ ì„ íƒí•©ë‹ˆë‹¤.")
                                selected_idx = 0
                    except Exception:
                        selected_idx = 0
                    
                    print(f"âœ… {selected_idx + 1}ë²ˆ ê°€ê²Œë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
                    place_href = final_places[selected_idx][2]
                else:
                    # ê²°ê³¼ê°€ 1ê°œì¸ ê²½ìš°
                    place_href = final_places[0][2]
                
                # ì¥ì†Œ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™
                if place_href.startswith('/'):
                    place_href = f"https://m.map.naver.com{place_href}"

                print(f"ğŸ“ ê°€ê²Œ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                await page.goto(place_href, wait_until="networkidle", timeout=30000)
                await page.wait_for_timeout(2000)

            # í˜„ì¬ URLì—ì„œ place ID ì¶”ì¶œ
            current_url = page.url
            place_id_match = re.search(r'/(?:restaurant|place)/(\d+)', current_url)

            if not place_id_match:
                print(f"âŒ í”Œë ˆì´ìŠ¤ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URL: {current_url}")
                await browser.close()
                return None

            place_id = place_id_match.group(1)

            # ê°€ê²Œ ì´ë¦„ ì¶”ì¶œ
            try:
                store_name_el = await page.locator('h1, .place_name, [class*="tit"]').first
                if await store_name_el.count() > 0:
                    store_name = await store_name_el.inner_text()
                    store_name = store_name.strip()
                elif 'store_name' not in locals():
                    store_name = query
            except:
                if 'store_name' not in locals():
                    store_name = query

            # ë¦¬ë·° í˜ì´ì§€ URL ìƒì„±
            review_url = f"https://m.place.naver.com/restaurant/{place_id}/review/visitor"

            print(f"âœ… ê°€ê²Œ ì°¾ê¸° ì™„ë£Œ: {store_name}")
            print(f"ğŸ“ ë¦¬ë·° URL: {review_url}")

            await browser.close()
            return (review_url, store_name)

        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            await browser.close()
            return None


async def crawl_naver_reviews(url: str, max_reviews: int = 50) -> List[Dict]:
    """
    ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤/ì§€ë„ ë¦¬ë·° í¬ë¡¤ë§ í•¨ìˆ˜ (Playwright ì‚¬ìš©)

    ë°ìŠ¤í¬í†±(map.naver.com)ê³¼ ëª¨ë°”ì¼(m.place.naver.com) URL ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    PlaywrightëŠ” ì‹¤ì œ ë¸Œë¼ìš°ì €ë¥¼ ìë™ìœ¼ë¡œ ì¡°ì‘í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

    Args:
        url: ë„¤ì´ë²„ ì§€ë„/í”Œë ˆì´ìŠ¤ URL (ì˜ˆ: https://m.place.naver.com/restaurant/31264425/review/visitor)
        max_reviews: ìˆ˜ì§‘í•  ìµœëŒ€ ë¦¬ë·° ê°œìˆ˜ (ê¸°ë³¸ê°’: 50ê°œ)

    Returns:
        ë¦¬ë·° ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        ê° ë”•ì…”ë„ˆë¦¬ í˜•ì‹:
        {
            'text': 'ì •ì œëœ ë¦¬ë·° ë³¸ë¬¸',
            'raw_text': 'ì›ë³¸ ë¦¬ë·° í…ìŠ¤íŠ¸',
            'rating': í‰ì (1-5),
            'date': 'ì‘ì„±ì¼ì',
            'source': 'naver'
        }
    """
    reviews = []  # ìˆ˜ì§‘í•œ ë¦¬ë·°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    # Playwright ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ (async withë¡œ ìë™ìœ¼ë¡œ ì¢…ë£Œë¨)
    async with async_playwright() as p:
        # URLì— 'm.place'ê°€ ìˆìœ¼ë©´ ëª¨ë°”ì¼ í˜ì´ì§€ë¡œ íŒë‹¨
        is_mobile = 'm.place.naver.com' in url

        # í¬ë¡¬ ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless=TrueëŠ” í™”ë©´ ì—†ì´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
        browser = await p.chromium.launch(headless=True)

        # ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„± (User-Agentì™€ í™”ë©´ í¬ê¸° ì„¤ì •)
        # User-Agent: ì›¹ì‚¬ì´íŠ¸ê°€ í¬ë¡¤ëŸ¬ë¥¼ ì°¨ë‹¨í•˜ì§€ ì•Šë„ë¡ ì‹¤ì œ ë¸Œë¼ìš°ì €ì¸ ê²ƒì²˜ëŸ¼ ìœ„ì¥
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1" if is_mobile else
                      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 375, "height": 812} if is_mobile else {"width": 1920, "height": 1080}
        )

        # ìƒˆ í˜ì´ì§€(íƒ­) ìƒì„±
        page = await context.new_page()

        try:
            print(f"í˜ì´ì§€ ì´ë™ ì¤‘: {url}")

            # í˜ì´ì§€ ë¡œë”© (wait_until="networkidle"ëŠ” ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°)
            # timeout=30000ì€ 30ì´ˆ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)
            await page.goto(url, wait_until="networkidle", timeout=30000)

            # ë™ì  ì»¨í…ì¸ (JavaScriptë¡œ ìƒì„±ë˜ëŠ” ë‚´ìš©)ê°€ ë¡œë”©ë  ë•Œê¹Œì§€ 3ì´ˆ ëŒ€ê¸°
            await page.wait_for_timeout(3000)

            print("ë¦¬ë·° ì¶”ì¶œ ì¤‘...")

            # ìŠ¤í¬ë¡¤ ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
            scroll_attempts = 0        # í˜„ì¬ê¹Œì§€ ì‹œë„í•œ ìŠ¤í¬ë¡¤ íšŸìˆ˜
            max_scroll_attempts = 20   # ìµœëŒ€ ìŠ¤í¬ë¡¤ ì‹œë„ íšŸìˆ˜ (ë¦¬ë·°ê°€ ë” ì•ˆë‚˜ì˜¤ë©´ ì¤‘ë‹¨)
            prev_count = 0             # ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘í•œ ë¦¬ë·° ê°œìˆ˜
            
            # ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„: ëª©í‘œ ë¦¬ë·° ê°œìˆ˜ì— ë„ë‹¬í•˜ê±°ë‚˜ ìŠ¤í¬ë¡¤ì´ ëë‚  ë•Œê¹Œì§€ ë°˜ë³µ
            while len(reviews) < max_reviews and scroll_attempts < max_scroll_attempts:
                # í˜ì´ì§€ì˜ ëª¨ë“  <ul> > <li> ìš”ì†Œ ì°¾ê¸° (ë„¤ì´ë²„ëŠ” ë¦¬ë·°ë¥¼ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œìœ¼ë¡œ í‘œì‹œ)
                review_elements = await page.locator("ul > li").all()

                temp_reviews = []  # ì´ë²ˆ ë£¨í”„ì—ì„œ ì°¾ì€ ë¦¬ë·°ë“¤ì„ ì„ì‹œ ì €ì¥

                # ê° ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œì„ ê²€ì‚¬í•˜ì—¬ ë¦¬ë·°ì¸ì§€ í™•ì¸
                for el in review_elements:
                    try:
                        # ìš”ì†Œì˜ í…ìŠ¤íŠ¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        text = await el.inner_text()

                        # í…ìŠ¤íŠ¸ê°€ ìˆê³  ìµœì†Œ ê¸¸ì´ë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ë¦¬ë·°ê°€ ì•„ë‹˜)
                        if text and len(text) > 10:
                            # UI ë©”íƒ€ë°ì´í„°ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ ë¦¬ë·° ë³¸ë¬¸ë§Œ ì¶”ì¶œ
                            cleaned_text = clean_review_text(text)

                            # ì •ì œ í›„ì—ë„ ìµœì†Œ ê¸¸ì´ë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
                            if not cleaned_text or len(cleaned_text) < 5:
                                continue

                            # ë¦¬ë·° ë°ì´í„° êµ¬ì¡° ìƒì„±
                            # - raw_text: ì›ë³¸ (LLM ë¶„ì„ìš©, í‰ì /ë‚ ì§œ ë“± ë©”íƒ€ë°ì´í„° í¬í•¨)
                            # - text: ì •ì œë³¸ (BERTopic í† í”½ ë¶„ì„ìš©, ìˆœìˆ˜ ë¦¬ë·° ë³¸ë¬¸ë§Œ)
                            review_data = {
                                "raw_text": text.strip(),      # ì›ë³¸ í…ìŠ¤íŠ¸ (ê³µë°± ì œê±°)
                                "text": cleaned_text,          # ì •ì œëœ í…ìŠ¤íŠ¸
                                "source": "naver"              # ì¶œì²˜ í‘œì‹œ
                            }

                            # í‰ì  ì¶”ì¶œ ì‹œë„ (ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©)
                            # ì˜ˆ: "5ì ", "4ê°œ" ê°™ì€ íŒ¨í„´ì—ì„œ ìˆ«ì ì¶”ì¶œ
                            rating_match = re.search(r'([1-5])(ì |ê°œ)', text)
                            if rating_match:
                                review_data['rating'] = int(rating_match.group(1))  # ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ
                            else:
                                review_data['rating'] = None  # í‰ì  ì •ë³´ ì—†ìŒ

                            # ì‘ì„±ì¼ì ì¶”ì¶œ ì‹œë„ (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
                            date_patterns = [
                                r'(\d{4}\.\d{1,2}\.\d{1,2})',  # "2024.01.15" í˜•ì‹
                                r'(\d{1,2}ê°œì›” ì „)',            # "3ê°œì›” ì „" í˜•ì‹
                                r'(\d{1,2}ì£¼ ì „)',              # "2ì£¼ ì „" í˜•ì‹
                                r'(\d{1,2}ì¼ ì „)',              # "5ì¼ ì „" í˜•ì‹
                            ]
                            for pattern in date_patterns:
                                date_match = re.search(pattern, text)
                                if date_match:
                                    review_data['date'] = date_match.group(1)
                                    break  # ì²« ë²ˆì§¸ë¡œ ë§¤ì¹­ëœ í˜•ì‹ ì‚¬ìš©

                            temp_reviews.append(review_data)

                    except Exception as e:
                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ìš”ì†Œ ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                        continue
                
                # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                # ì´ë¯¸ ìˆ˜ì§‘í•œ ë¦¬ë·°ì˜ í…ìŠ¤íŠ¸ë¥¼ Setì— ì €ì¥í•˜ì—¬ ë¹ ë¥¸ ì¤‘ë³µ ê²€ì‚¬
                unique_texts = set([r['raw_text'] for r in reviews])
                for r in temp_reviews:
                    if r['raw_text'] not in unique_texts:  # ì¤‘ë³µì´ ì•„ë‹ˆë©´
                        reviews.append(r)                   # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                        unique_texts.add(r['raw_text'])     # Setì—ë„ ì¶”ê°€

                current_count = len(reviews)
                print(f"í˜„ì¬ê¹Œì§€ {current_count}ê°œ ë¦¬ë·° ìˆ˜ì§‘ (ì‹œë„ {scroll_attempts + 1}íšŒ)...")

                # ìƒˆë¡œìš´ ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤ ì‹œë„ íšŸìˆ˜ ì¦ê°€
                if current_count == prev_count:
                    scroll_attempts += 1  # ë¦¬ë·°ê°€ ì•ˆëŠ˜ì–´ë‚˜ë©´ ì¹´ìš´íŠ¸ ì¦ê°€
                else:
                    scroll_attempts = 0    # ìƒˆ ë¦¬ë·°ê°€ ìˆìœ¼ë©´ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                    prev_count = current_count

                # í˜ì´ì§€ ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤ (ìƒˆë¡œìš´ ë¦¬ë·° ë¡œë”© ìœ ë„)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(2000)  # 2ì´ˆ ëŒ€ê¸° (ìƒˆ ì½˜í…ì¸  ë¡œë”© ì‹œê°„)

                # "ë”ë³´ê¸°" ë²„íŠ¼ì´ ìˆìœ¼ë©´ í´ë¦­ ì‹œë„
                try:
                    more_buttons = await page.locator("button:has-text('ë”ë³´ê¸°'), a:has-text('ë”ë³´ê¸°')").all()
                    if more_buttons:
                        await more_buttons[0].click()  # ì²« ë²ˆì§¸ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
                        await page.wait_for_timeout(1500)  # 1.5ì´ˆ ëŒ€ê¸°
                        scroll_attempts = 0  # ë²„íŠ¼ í´ë¦­ ì„±ê³µ ì‹œ ìŠ¤í¬ë¡¤ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                except:
                    pass  # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

            print(f"âœ… ì´ {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            # í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()  # ìƒì„¸ ì˜¤ë¥˜ ë‚´ì—­ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        finally:
            # ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ë¸Œë¼ìš°ì € ì¢…ë£Œ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
            await browser.close()

    # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ ë¦¬ë·° ë°˜í™˜ (ì´ˆê³¼ ìˆ˜ì§‘í•œ ê²½ìš° ì˜ë¼ëƒ„)
    return reviews[:max_reviews]


async def crawl_by_search(query: str, max_reviews: int = 50) -> Tuple[List[Dict], Optional[str]]:
    """
    ê°€ê²Œ ì´ë¦„/ì£¼ì†Œë¡œ ê²€ìƒ‰í•˜ì—¬ ìë™ìœ¼ë¡œ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ì˜¬ì¸ì› í•¨ìˆ˜

    ì‚¬ìš©ì ì…ì¥ì—ì„œ ê°€ì¥ í¸ë¦¬í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤. URLì„ ëª°ë¼ë„ ê°€ê²Œ ì´ë¦„ë§Œ ì…ë ¥í•˜ë©´
    ìë™ìœ¼ë¡œ ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê²€ìƒ‰í•˜ê³  ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        query: ê°€ê²Œ ì´ë¦„ ë˜ëŠ” ì£¼ì†Œ (ì˜ˆ: "ë°”ëŒë‚œ ì–¼í° ìˆ˜ì œë¹„ ë²”ê³„ì ", "ì„œìš¸ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123")
        max_reviews: ìˆ˜ì§‘í•  ìµœëŒ€ ë¦¬ë·° ê°œìˆ˜ (ê¸°ë³¸ê°’: 50ê°œ)

    Returns:
        (ë¦¬ë·° ë¦¬ìŠ¤íŠ¸, ê°€ê²Œëª…) íŠœí”Œ
        - ë¦¬ë·° ë¦¬ìŠ¤íŠ¸: [{'text': ..., 'rating': ..., 'date': ..., 'source': 'naver'}, ...]
        - ê°€ê²Œëª…: ìë™ìœ¼ë¡œ ì¶”ì¶œëœ ì •í™•í•œ ê°€ê²Œ ì´ë¦„

    Example:
        >>> reviews, store_name = await crawl_by_search("ë°”ëŒë‚œ ì–¼í° ìˆ˜ì œë¹„ ë²”ê³„ì ", max_reviews=30)
        >>> print(f"{store_name}ì˜ ë¦¬ë·° {len(reviews)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ!")
    """
    print("=" * 60)
    print("ğŸ” ìë™ ê²€ìƒ‰ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 60)

    # 1ë‹¨ê³„: ë„¤ì´ë²„ ì§€ë„ì—ì„œ ê°€ê²Œ ê²€ìƒ‰
    search_result = await search_place_and_get_url(query)

    if not search_result:
        print("âŒ ê°€ê²Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return ([], None)

    review_url, store_name = search_result

    # 2ë‹¨ê³„: ë¦¬ë·° í¬ë¡¤ë§
    print(f"\nğŸ“¥ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
    reviews = await crawl_naver_reviews(review_url, max_reviews=max_reviews)

    print("=" * 60)
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {store_name}")
    print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë¦¬ë·°: {len(reviews)}ê°œ")
    print("=" * 60)

    return (reviews, store_name)


# ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì‘ë™í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import sys

    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    if len(sys.argv) > 1:
        # ëª…ë ¹ì¤„ ì¸ìë¡œ ê²€ìƒ‰ì–´ ë°›ê¸°: python playwright_crawler.py "ë°”ëŒë‚œ ì–¼í° ìˆ˜ì œë¹„"
        query = " ".join(sys.argv[1:])
    else:
        # ëŒ€í™”í˜•ìœ¼ë¡œ ê²€ìƒ‰ì–´ ì…ë ¥ë°›ê¸°
        query = input("\nğŸ” ê²€ìƒ‰í•  ê°€ê²Œ ì´ë¦„ì´ë‚˜ ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    if not query:
        print("âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # ìë™ ê²€ìƒ‰ í¬ë¡¤ë§ ì‹¤í–‰
    reviews, store_name = asyncio.run(crawl_by_search(query, max_reviews=20))

    if not reviews:
        print("âŒ ë¦¬ë·° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # ê²°ê³¼ ì¶œë ¥ (ì²˜ìŒ 5ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°)
    print(f"\n=== ìˆ˜ì§‘ ê²°ê³¼ ({store_name}) ===")
    for i, review in enumerate(reviews[:5], 1):
        print(f"\n[ë¦¬ë·° {i}]")
        print(f"ë‚´ìš©: {review.get('text', 'N/A')[:100]}...")
        print(f"í‰ì : {review.get('rating', 'N/A')}")
        print(f"ì‘ì„±ì¼: {review.get('date', 'N/A')}")
