"""
ë„¤ì´ë²„ ë° ì¹´ì¹´ì˜¤ë§µ ë¦¬ë·° í¬ë¡¤ë§ ì„œë¹„ìŠ¤
"""
import asyncio
import re
import sys
import threading
from typing import List, Dict, Optional, Tuple
from playwright.async_api import async_playwright
from app.utils.logger import get_logger

logger = get_logger(__name__)

class CrawlerService:
    """
    ë„¤ì´ë²„ì™€ ì¹´ì¹´ì˜¤ë§µì—ì„œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """

    @staticmethod
    def _clean_review_text(text: str) -> str:
        """
        ë¦¬ë·° í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ UI ìš”ì†Œ(ë©”íƒ€ë°ì´í„°)ë¥¼ ì œê±°í•˜ê³  ì •ì œí•©ë‹ˆë‹¤.
        """
        lines = text.split('\n')
        noise_patterns = [
            r'^ë¦¬ë·°\s+\d+', r'^ì‚¬ì§„\s+\d+', r'íŒ”ë¡œì›Œ?\s+\d+', r'^\d+\s*íŒ”ë¡œìš°',
            r'ë°©ë¬¸ì¼\s+\d+\.\d+\.', r'\d{4}ë…„\s+\d{1,2}ì›”\s+\d{1,2}ì¼',
            r'[ì¼ì›”í™”ìˆ˜ëª©ê¸ˆí† ]ìš”ì¼', r'\d+ë²ˆì§¸\s+ë°©ë¬¸', r'ì¸ì¦\s+ìˆ˜ë‹¨',
            r'ì˜ìˆ˜ì¦|ê²°ì œë‚´ì—­', r'ë”\s*ë³´ê¸°', r'í¼ì³ë³´ê¸°', r'ë°˜ì‘\s+ë‚¨ê¸°ê¸°',
            r'ê°œì˜\s+ë¦¬ë·°ê°€\s+ë”\s+ìˆìŠµë‹ˆë‹¤', r'^\s*[+â€»]\d+\s*$',
            r'ì˜ˆì•½\s+ì—†ì´\s+ì´ìš©', r'ëŒ€ê¸°\s+ì‹œê°„\s+ë°”ë¡œ\s+ì…ì¥',
            r'[ì €ì ]ì‹¬ì—?\s+ë°©ë¬¸', r'ì¼ìƒ|ì¹œëª©|ë°ì´íŠ¸|ë‚˜ë“¤ì´',
            r'í˜¼ì|ì—°ì¸ãƒ»ë°°ìš°ì|ì¹œêµ¬|ê°€ì¡±|ì•„ì´', r'@\w+'
        ]
        
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if not line: continue
            
            is_noise = False
            for pattern in noise_patterns:
                if re.search(pattern, line):
                    is_noise = True
                    break
            
            # ì§§ì€ UI í…ìŠ¤íŠ¸ í•„í„°ë§
            short_keywords = ['ìŒì‹ì´ ë§›ìˆì–´ìš”', 'ë§¤ì¥ì´ ì²­ê²°í•´ìš”', 'ì¹œì ˆí•´ìš”', 'ê°€ì„±ë¹„ê°€ ì¢‹ì•„ìš”']
            if line.startswith('"') and any(k in line for k in short_keywords):
                continue
                
            if re.match(r'^\d+$', line): # ìˆ«ìë§Œ ìˆëŠ” ì¤„
                continue
                
            if not is_noise and len(line) > 3:
                cleaned_lines.append(line)
                
        review_text = ' '.join(cleaned_lines)
        review_text = re.sub(r'[+â€»~]{2,}', '', review_text)
        review_text = re.sub(r'\s+', ' ', review_text)
        return review_text.strip()

    async def crawl_naver(self, query: str, max_reviews: int = 50) -> List[Dict]:
        """
        ë„¤ì´ë²„ í”Œë ˆì´ìŠ¤ì—ì„œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        logger.info(f"ğŸš€ [Naver] Searching for: {query}")
        reviews = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
                viewport={"width": 375, "height": 812}
            )
            page = await context.new_page()
            
            try:
                # 1. ê²€ìƒ‰ ë° URL íšë“
                search_url = f"https://m.map.naver.com/search2/search.naver?query={query}"
                await page.goto(search_url, wait_until="networkidle", timeout=30000)
                await page.wait_for_timeout(2000)
                
                place_id = None
                current_url = page.url
                
                # ìƒì„¸ í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™í–ˆëŠ”ì§€ í™•ì¸
                if "m.place.naver.com" in current_url and ("/restaurant/" in current_url or "/place/" in current_url):
                    match = re.search(r'/(?:restaurant|place)/(\d+)', current_url)
                    if match: place_id = match.group(1)
                else:
                    # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ í•­ëª© ì„ íƒ
                    try:
                        first_link = page.locator('a[href*="/place/"], a[href*="/restaurant/"]').first
                        if await first_link.count() > 0:
                            href = await first_link.get_attribute('href')
                            match = re.search(r'/(?:restaurant|place)/(\d+)', href)
                            if match: place_id = match.group(1)
                    except:
                        pass

                if not place_id:
                    logger.warning("[Naver] Could not find place ID.")
                    return []

                # 2. ë¦¬ë·° í˜ì´ì§€ ì´ë™
                review_url = f"https://m.place.naver.com/restaurant/{place_id}/review/visitor"
                logger.info(f"Go to Review Page: {review_url}")
                await page.goto(review_url, wait_until="networkidle", timeout=30000)
                await page.wait_for_timeout(3000)

                # 3. í¬ë¡¤ë§ ë£¨í”„
                scroll_attempts = 0
                max_scrolls = 20
                prev_count = 0
                
                while len(reviews) < max_reviews and scroll_attempts < max_scrolls:
                    elements = await page.locator("ul > li").all()
                    
                    temp_reviews = []
                    for el in elements:
                        try:
                            text = await el.inner_text()
                            if text and len(text) > 10:
                                cleaned = self._clean_review_text(text)
                                if len(cleaned) < 5: continue
                                
                                # í‰ì  ì¶”ì¶œ
                                rating = None
                                match = re.search(r'([1-5])(ì |ê°œ)', text)
                                if match: rating = int(match.group(1))

                                # ë‚ ì§œ ê°„ë‹¨ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë°œê²¬ë˜ëŠ” ë‚ ì§œ íŒ¨í„´)
                                date = None
                                date_match = re.search(r'(\d{4}\.\d{1,2}\.\d{1,2})', text)
                                if date_match: date = date_match.group(1)

                                temp_reviews.append({
                                    "raw_text": text.strip(),
                                    "text": cleaned,
                                    "rating": rating,
                                    "date": date,
                                    "source": "naver"
                                })
                        except: continue

                    # ì¤‘ë³µ ì œê±° ë° ì¶”ê°€
                    unique_texts = set(r['raw_text'] for r in reviews)
                    for r in temp_reviews:
                        if r['raw_text'] not in unique_texts:
                            reviews.append(r)
                            unique_texts.add(r['raw_text'])
                    
                    if len(reviews) == prev_count:
                        scroll_attempts += 1
                        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„
                        try:
                            btn = page.locator("button:has-text('ë”ë³´ê¸°'), a:has-text('ë”ë³´ê¸°')").first
                            if await btn.count() > 0:
                                await btn.click()
                                await page.wait_for_timeout(1000)
                                scroll_attempts = 0
                        except: pass
                    else:
                        scroll_attempts = 0
                        prev_count = len(reviews)

                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await page.wait_for_timeout(1500)

                logger.info(f"âœ… [Naver] Collected {len(reviews)} reviews")
                
            except Exception as e:
                logger.error(f"âŒ [Naver] Crawling error: {e}")
            finally:
                await browser.close()
                
        return reviews[:max_reviews]

    async def crawl_kakao(self, query: str, max_reviews: int = 50) -> List[Dict]:
        """
        ì¹´ì¹´ì˜¤ë§µì—ì„œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        """
        logger.info(f"ğŸš€ [Kakao] Searching for: {query}")
        reviews = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1",
                viewport={"width": 375, "height": 812}
            )
            page = await context.new_page()
            
            try:
                # 1. ê²€ìƒ‰
                search_url = f"https://m.map.kakao.com/actions/searchView?q={query}"
                await page.goto(search_url, wait_until="networkidle", timeout=30000)
                await page.wait_for_timeout(2000)
                
                # ì²« ë²ˆì§¸ ê²°ê³¼ ì„ íƒ
                first_item = page.locator('li[data-id]').first
                if await first_item.count() == 0:
                    logger.warning("[Kakao] No search results found.")
                    return []
                    
                data_id = await first_item.get_attribute("data-id")
                review_url = f"https://place.map.kakao.com/{data_id}#review"
                
                # 2. ë¦¬ë·° í˜ì´ì§€ ì´ë™
                await page.goto(review_url, wait_until="networkidle", timeout=30000)
                await page.wait_for_timeout(3000)
                
                # 3. í¬ë¡¤ë§ ë£¨í”„
                scroll_attempts = 0
                max_scrolls = 15
                prev_count = 0
                
                while len(reviews) < max_reviews and scroll_attempts < max_scrolls:
                    elements = await page.locator("ul.list_review > li").all()
                    
                    temp_reviews = []
                    for el in elements:
                        try:
                            text_el = el.locator("p.desc_review").first
                            if await text_el.count() == 0: continue
                            
                            # ë”ë³´ê¸° í´ë¦­
                            more = text_el.locator(".btn_more").first
                            if await more.count() > 0 and await more.is_visible():
                                await more.click(timeout=1000)
                            
                            text = await text_el.inner_text()
                            text = text.replace("ë”ë³´ê¸°", "").strip()
                            if not text: continue
                            
                            # ë³„ì 
                            rating = None
                            try:
                                spans = await el.locator(".starred_grade .screen_out").all()
                                for s in spans:
                                    st = await s.inner_text()
                                    if st.replace('.','').isdigit():
                                        rating = int(float(st))
                                        break
                            except: pass

                            # ë‚ ì§œ
                            date = None
                            try:
                                de = el.locator(".txt_date").first
                                if await de.count() > 0: date = await de.inner_text()
                            except: pass
                            
                            temp_reviews.append({
                                "raw_text": text,
                                "text": text, # ì¹´ì¹´ì˜¤ëŠ” ë¹„êµì  ê¹¨ë—í•¨
                                "rating": rating,
                                "date": date,
                                "source": "kakao"
                            })
                        except: continue
                        
                    unique_texts = set(r['raw_text'] for r in reviews)
                    for r in temp_reviews:
                        if r['raw_text'] not in unique_texts:
                            reviews.append(r)
                            unique_texts.add(r['raw_text'])
                            
                    if len(reviews) == prev_count:
                        scroll_attempts += 1
                        # ë”ë³´ê¸° ë²„íŠ¼ (í˜ì´ì§€ í•˜ë‹¨)
                        try:
                            more_link = page.locator("a.link_more:has-text('í›„ê¸° ë”ë³´ê¸°')").first
                            if await more_link.count() > 0 and await more_link.is_visible():
                                await more_link.click()
                                await page.wait_for_timeout(1000)
                                scroll_attempts = 0
                        except: pass
                    else:
                        scroll_attempts = 0
                        prev_count = len(reviews)
                        
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await page.wait_for_timeout(1500)
                    
                logger.info(f"âœ… [Kakao] Collected {len(reviews)} reviews")
                
            except Exception as e:
                logger.error(f"âŒ [Kakao] Crawling error: {e}")
            finally:
                await browser.close()
                
        return reviews[:max_reviews]
    
    async def collect_all_reviews(self, store_name: str, address: str) -> List[Dict]:
        """
        ë„¤ì´ë²„ì™€ ì¹´ì¹´ì˜¤ë§µ ë¦¬ë·°ë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        ê°€ê²Œ ì´ë¦„ê³¼ ì£¼ì†Œë¥¼ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        
        Windowsì—ì„œëŠ” Uvicornì˜ SelectorEventLoopê³¼ Playwrightì˜ ProactorEventLoop
        ì¶©ëŒì„ í”¼í•˜ê¸° ìœ„í•´, ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒˆë¡œìš´ ProactorEventLoopì„ ìƒì„±í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        query = f"{address} {store_name}"
        logger.info(f"ğŸ” Starting concurrent crawling for: {query}")

        async def _crawl_all():
            naver_task = asyncio.create_task(self.crawl_naver(query))
            kakao_task = asyncio.create_task(self.crawl_kakao(query))
            return await asyncio.gather(naver_task, kakao_task)

        if sys.platform == 'win32':
            # Windows: Uvicorn uses SelectorEventLoop which can't spawn subprocesses.
            # Run Playwright in a dedicated thread with its own ProactorEventLoop.
            result_container = {}

            def _run_in_thread():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    # ProactorEventLoop is the default on Windows when creating a new loop
                    # but let's be explicit
                    if not isinstance(loop, asyncio.ProactorEventLoop):
                        loop.close()
                        loop = asyncio.ProactorEventLoop()
                        asyncio.set_event_loop(loop)
                    result_container['result'] = loop.run_until_complete(_crawl_all())
                except Exception as e:
                    result_container['error'] = e
                finally:
                    loop.close()

            thread = threading.Thread(target=_run_in_thread)
            thread.start()
            
            # awaitë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì¸ ë£¨í”„ë¥¼ ë¸”ë¡œí‚¹í•˜ì§€ ì•Šê³  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
            loop = asyncio.get_running_loop()
            await loop.run_in_executor(None, thread.join)

            if 'error' in result_container:
                raise result_container['error']
            results = result_container['result']
        else:
            # Linux/Mac: ì´ë²¤íŠ¸ ë£¨í”„ ì¶©ëŒ ì—†ìŒ, ì§ì ‘ ì‹¤í–‰
            results = await _crawl_all()

        all_reviews = results[0] + results[1]
        logger.info(f"ğŸ“Š Total reviews collected: {len(all_reviews)} (Naver: {len(results[0])}, Kakao: {len(results[1])})")

        return all_reviews
