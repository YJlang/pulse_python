"""
BERTopic ê¸°ë°˜ ë¦¬ë·° ë¶„ì„ ë° í† í”½ ëª¨ë¸ë§ ì„œë¹„ìŠ¤
"""
from typing import List, Dict, Any
from collections import Counter
import torch
from kiwipiepy import Kiwi
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from app.utils.logger import get_logger

logger = get_logger(__name__)

class AnalysisService:
    """
    NLP ë¶„ì„ ë° í† í”½ ëª¨ë¸ë§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ì‹±ê¸€í†¤ íŒ¨í„´ê³¼ ìœ ì‚¬í•˜ê²Œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    _instance = None
    
    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    STOPWORDS = {
        'ë¦¬ë·°', 'ì‚¬ì§„', 'íŒ”ë¡œìš°', 'íŒ”ë¡œì›Œ', 'ë°©ë¬¸', 'ì˜ˆì•½', 'ì´ìš©', 'ëŒ€ê¸°', 'ì‹œê°„',
        'ì…ì¥', 'ë°˜ì‘', 'ì¸ì¦', 'ìˆ˜ë‹¨', 'ì˜ìˆ˜ì¦', 'ê²°ì œ', 'ë‚´ì—­',
        'ì¼ìš”ì¼', 'ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼',
        'ë…„', 'ì›”', 'ì¼', 'ë²ˆì§¸', 'ì €ë…', 'ì ì‹¬', 'ì•„ì¹¨', 'ì˜¤ì „', 'ì˜¤í›„',
        'ì¼ìƒ', 'ì¹œëª©', 'ë°ì´íŠ¸', 'ë‚˜ë“¤ì´', 'í˜¼ì', 'ì¹œêµ¬', 'ê°€ì¡±', 'ì—°ì¸', 'ë°°ìš°ì', 'ì•„ì´', 'ë™ë£Œ',
        'ê°œ', 'ê³³', 'ë”', 'ìˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'í•©ë‹ˆë‹¤', 'ì´ë‹¤', 'ì…ë‹ˆë‹¤',
        'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ë°', 'ìœ„í•´', 'í†µí•´', 'í•˜ë‚˜', 'ê°€ì§€',
        'ì¸ì›', 'ì„ íƒ', 'í‚¤ì›Œë“œ', 'ì¡°íšŒ', 'ì—…ì²´', 'ì¥ì†Œ', 'í…Œë§ˆ', 'ë¦¬ìŠ¤íŠ¸'
    }

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(AnalysisService, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def initialize(self):
        """
        ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤. (App Startup ì‹œ í˜¸ì¶œ ê¶Œì¥)
        """
        if self.initialized:
            return

        logger.info("â³ [AnalysisService] Loading NLP models... (This may take a while)")
        
        # 1. Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ
        self.kiwi = Kiwi()
        logger.info("   âœ… Kiwi loaded")
        
        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (GPU í™•ì¸)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"   Using device: {device}")
        self.embedding_model = SentenceTransformer("jhgan/ko-sbert-nli", device=device)
        logger.info("   âœ… Embedding model loaded")
        
        self.initialized = True
        logger.info("âœ… [AnalysisService] Initialization complete")

    def _preprocess(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        """
        tokens = self.kiwi.tokenize(text)
        results = []
        for token in tokens:
            if token.tag in ['NNG', 'NNP']:
                word = token.form
                if len(word) > 1 and word not in self.STOPWORDS:
                    results.append(word)
        return results

    def run_analysis(self, reviews: List[Dict], n_topics: int = None) -> Dict[str, Any]:
        """
        ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            {
                'topics': {topic_id: [keyword1, keyword2...]},
                'topic_counts': {topic_id: count},
                'reviews_with_topics': [review_dict_with_topic_id],
                'docs_count': int
            }
        """
        if not reviews:
            return {"error": "ë¦¬ë·° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        if not self.initialized:
            self.initialize()

        logger.info(f"ğŸ“Š Analyzing {len(reviews)} reviews...")

        # 1. ì „ì²˜ë¦¬
        processed_docs = []
        valid_reviews = []
        
        for r in reviews:
            text = r.get('text', r.get('raw_text', ''))
            tokens = self._preprocess(text)
            if tokens:
                processed_docs.append(' '.join(tokens))
                # ì›ë³¸ ë¦¬ë·°ì— í† í° ì •ë³´ ì¶”ê°€ (ë‚˜ì¤‘ì— í‚¤ì›Œë“œ ì¶”ì¶œìš©)
                r['tokens'] = tokens
                valid_reviews.append(r)
        
        if not processed_docs:
            return {"error": "ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # 2. í† í”½ ëª¨ë¸ë§ (BERTopic)
        # HDBSCAN ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
        try:
            import hdbscan
            has_hdbscan = True
        except ImportError:
            has_hdbscan = False

        # ëª¨ë¸ ì„¤ì •
        if n_topics or not has_hdbscan:
            # í† í”½ ìˆ˜ ì§€ì • ë˜ëŠ” KMeans ì‚¬ìš© ì‹œ
            n_clusters = n_topics if n_topics else max(3, min(len(processed_docs) // 10, 10))
            cluster_model = KMeans(n_clusters=n_clusters, random_state=42)
            topic_model = BERTopic(
                embedding_model=self.embedding_model,
                hdbscan_model=cluster_model,
                verbose=False,
                min_topic_size=3
            )
        else:
            # HDBSCAN ìë™ í´ëŸ¬ìŠ¤í„°ë§
            topic_model = BERTopic(
                embedding_model=self.embedding_model,
                verbose=False,
                min_topic_size=3
            )

        topics, _ = topic_model.fit_transform(processed_docs)
        
        # 3. í† í”½ ì •ë³´ ë§¤í•‘ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
        topic_counts = Counter(topics)
        keywords_map = {}
        
        # ê° ë¦¬ë·°ì— í† í”½ ID í• ë‹¹
        for i, review in enumerate(valid_reviews):
            review['topic'] = int(topics[i])

        # í† í”½ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜)
        # BERTopic ë‚´ì¥ í•¨ìˆ˜ get_topic()ì„ ì¨ë„ ë˜ì§€ë§Œ, ì»¤ìŠ¤í…€ ë¡œì§(ëª…ì‚¬ë§Œ)ì´ ë” ì •í™•í•  ìˆ˜ ìˆìŒ
        unique_topics = set(topics)
        if -1 in unique_topics: unique_topics.remove(-1) # ì•„ì›ƒë¼ì´ì–´ ì œì™¸
        
        for t_id in unique_topics:
            # í•´ë‹¹ í† í”½ì˜ ëª¨ë“  í† í° ìˆ˜ì§‘
            all_tokens = []
            for r in valid_reviews:
                if r['topic'] == t_id:
                    all_tokens.extend(r['tokens'])
            
            # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ
            top_5 = [word for word, count in Counter(all_tokens).most_common(5)]
            keywords_map[int(t_id)] = top_5

        logger.info(f"âœ… Analysis done. Found {len(unique_topics)} topics.")
        
        return {
            "topics": keywords_map,
            "topic_counts": dict(topic_counts),
            "reviews_with_topics": valid_reviews,
            "docs_count": len(processed_docs)
        }
