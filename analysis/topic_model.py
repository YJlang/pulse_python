from typing import List, Dict, Any
import pandas as pd
from kiwipiepy import Kiwi
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from collections import Counter
import os
import torch

# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ìš©)
kiwi = Kiwi()

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸: ë„¤ì´ë²„ ë¦¬ë·° UI ë©”íƒ€ë°ì´í„° ë° ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´
STOPWORDS = {
    # UI ë©”íƒ€ë°ì´í„°
    'ë¦¬ë·°', 'ì‚¬ì§„', 'íŒ”ë¡œìš°', 'íŒ”ë¡œì›Œ', 'ë°©ë¬¸', 'ì˜ˆì•½', 'ì´ìš©', 'ëŒ€ê¸°', 'ì‹œê°„',
    'ì…ì¥', 'ë°˜ì‘', 'ì¸ì¦', 'ìˆ˜ë‹¨', 'ì˜ìˆ˜ì¦', 'ê²°ì œ', 'ë‚´ì—­',
    # ìš”ì¼ ë° ë‚ ì§œ
    'ì¼ìš”ì¼', 'ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼',
    'ë…„', 'ì›”', 'ì¼', 'ë²ˆì§¸',
    # ë°©ë¬¸ ê´€ë ¨
    'ì €ë…', 'ì ì‹¬', 'ì•„ì¹¨', 'ì˜¤ì „', 'ì˜¤í›„',
    # ë™ë°˜ì/ëª©ì  íƒœê·¸
    'ì¼ìƒ', 'ì¹œëª©', 'ë°ì´íŠ¸', 'ë‚˜ë“¤ì´', 'í˜¼ì', 'ì¹œêµ¬', 'ê°€ì¡±', 'ì—°ì¸', 'ë°°ìš°ì', 'ì•„ì´', 'ë™ë£Œ',
    # ì¼ë°˜ ë¶ˆìš©ì–´
    'ê°œ', 'ê³³', 'ë”', 'ìˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'í•©ë‹ˆë‹¤', 'ì´ë‹¤', 'ì…ë‹ˆë‹¤',
    'ê²ƒ', 'ê±°', 'ìˆ˜', 'ë“±', 'ë•Œ', 'ë°', 'ìœ„í•´', 'í†µí•´', 'í•˜ë‚˜', 'ê°€ì§€',
    # ìˆ˜ì¹˜ ë° ê¸°íƒ€
    'ì¸ì›', 'ì„ íƒ', 'í‚¤ì›Œë“œ', 'ì¡°íšŒ', 'ì—…ì²´', 'ì¥ì†Œ', 'í…Œë§ˆ', 'ë¦¬ìŠ¤íŠ¸',
}

def preprocess_text(text: str) -> List[str]:
    """
    Kiwië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸

    Returns:
        ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ (2ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸)
    """
    results = []
    # í…ìŠ¤íŠ¸ë¥¼ í† í°(ë‹¨ì–´)ìœ¼ë¡œ ë¶„ë¦¬
    tokens = kiwi.tokenize(text)
    for token in tokens:
        # NNG(ì¼ë°˜ ëª…ì‚¬)ì™€ NNP(ê³ ìœ  ëª…ì‚¬)ë§Œ ì¶”ì¶œ
        if token.tag in ['NNG', 'NNP']:
            word = token.form
            # í•œ ê¸€ì ë‹¨ì–´ëŠ” ì œì™¸ (ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ í•„í„°ë§)
            # ë¶ˆìš©ì–´ ì œì™¸
            if len(word) > 1 and word not in STOPWORDS:
                results.append(word)
    return results


def run_topic_model(reviews: List[Dict], n_topics: int = None, output_dir: str = "./output") -> Dict[str, Any]:
    """
    ë¦¬ë·° ë°ì´í„°ì— ëŒ€í•´ BERTopic í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ê³  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Market-Compass êµ¬í˜„ ë°©ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

    Args:
        reviews: 'text' ë˜ëŠ” 'raw_text' í‚¤ë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        n_topics: ì¶”ì¶œí•  í† í”½ ê°œìˆ˜ (Noneì´ë©´ ìë™ìœ¼ë¡œ ê²°ì •)
        output_dir: ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        í† í”½, í‚¤ì›Œë“œ, íŒŒì¼ ê²½ë¡œ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
    """
    if not reviews:
        return {"error": "No reviews provided"}

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ“Š Processing {len(reviews)} reviews...")

    # 1ë‹¨ê³„: ì „ì²˜ë¦¬ - ëª…ì‚¬ ì¶”ì¶œ
    print("ğŸ” Step 1: Preprocessing with Kiwi...")
    processed_data = []
    for r in reviews:
        # ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ ('text' ë˜ëŠ” 'raw_text' í‚¤ ì‚¬ìš©)
        text = r.get('text', r.get('raw_text', ''))
        # Kiwië¡œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
        tokens = preprocess_text(text)
        if tokens:
            processed_data.append({
                'original_text': text,          # ì›ë³¸ í…ìŠ¤íŠ¸
                'tokens': tokens,               # ì¶”ì¶œëœ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸
                'document': ' '.join(tokens)    # BERTopic ì…ë ¥ìš© ë¬¸ìì—´
            })

    if not processed_data:
        return {"error": "No valid text found after preprocessing"}

    print(f"âœ… Preprocessed {len(processed_data)} documents")
    
    # 2ë‹¨ê³„: BERTopic ëª¨ë¸ë§ (GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPU í™œìš©)
    print("\nğŸ¤– Step 2: BERTopic modeling...")
    # CUDA(GPU) ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Using device: {device.upper()}")

    # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (GPU/CPU ì„¤ì • í¬í•¨)
    embedding_model = SentenceTransformer("jhgan/ko-sbert-nli", device=device)

    # BERTopicì— ì…ë ¥í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
    documents = [d['document'] for d in processed_data]

    # HDBSCAN í´ëŸ¬ìŠ¤í„°ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    # HDBSCANì´ ì—†ìœ¼ë©´ KMeansë¡œ ëŒ€ì²´
    try:
        import hdbscan
        has_hdbscan = True
    except ImportError:
        has_hdbscan = False
        print("   âš ï¸ HDBSCAN not available, using KMeans")

    # BERTopic ëª¨ë¸ ì´ˆê¸°í™”
    # n_topicsê°€ ì§€ì •ë˜ì—ˆê±°ë‚˜ HDBSCANì´ ì—†ìœ¼ë©´ KMeans ì‚¬ìš©
    if n_topics or not has_hdbscan:
        # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ ê²°ì •: ì§€ì •ëœ ê°’ ë˜ëŠ” ë¬¸ì„œ ìˆ˜ì˜ 1/10 (ìµœì†Œ 3, ìµœëŒ€ 10)
        n_clusters = n_topics if n_topics else max(3, min(len(documents) // 10, 10))
        cluster_model = KMeans(n_clusters=n_clusters, random_state=42)
        topic_model = BERTopic(
            embedding_model=embedding_model,     # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
            hdbscan_model=cluster_model,         # KMeans í´ëŸ¬ìŠ¤í„°ë§ ì‚¬ìš©
            verbose=True,                        # ì§„í–‰ ìƒí™© ì¶œë ¥
            min_topic_size=3                     # í† í”½ë‹¹ ìµœì†Œ ë¬¸ì„œ ìˆ˜
        )
    else:
        # HDBSCAN ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ í´ëŸ¬ìŠ¤í„°ë§
        topic_model = BERTopic(
            embedding_model=embedding_model,
            verbose=True,
            min_topic_size=3
        )

    # ëª¨ë¸ í•™ìŠµ ë° í† í”½ ì˜ˆì¸¡
    # topics: ê° ë¬¸ì„œì˜ í† í”½ ë²ˆí˜¸, probs: ê° í† í”½ì— ì†í•  í™•ë¥ 
    topics, probs = topic_model.fit_transform(documents)

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì •ë¦¬
    df = pd.DataFrame(processed_data)
    df['topic'] = topics  # ê° ë¬¸ì„œì— í† í”½ ë²ˆí˜¸ ì¶”ê°€

    # -1ì€ ì•„ì›ƒë¼ì´ì–´(ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë¬¸ì„œ)ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ì¹´ìš´íŠ¸
    print(f"âœ… Topic modeling complete - Found {len(set(topics)) - (1 if -1 in topics else 0)} topics")
    
    # 3ë‹¨ê³„: ì»¤ìŠ¤í…€ í‚¤ì›Œë“œ ì¶”ì¶œ (Market-Compass ë°©ì‹)
    # ê° í† í”½ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ 5ê°œë¥¼ í‚¤ì›Œë“œë¡œ ì„ ì •
    print("\nğŸ“Œ Step 3: Extracting custom keywords...")
    topic_keywords = {}   # í† í”½ë³„ í‚¤ì›Œë“œ ì €ì¥
    topic_counts = {}     # í† í”½ë³„ ë¬¸ì„œ ìˆ˜ ì €ì¥

    for topic_num in df['topic'].unique():
        if topic_num == -1:
            continue  # ì•„ì›ƒë¼ì´ì–´(ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë¬¸ì„œ)ëŠ” ê±´ë„ˆë›°ê¸°

        # í•´ë‹¹ í† í”½ì— ì†í•œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        topic_docs = df[df['topic'] == topic_num]
        all_tokens = []
        # í•´ë‹¹ í† í”½ì˜ ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
        for tokens in topic_docs['tokens']:
            all_tokens.extend(tokens)

        # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°í•˜ê³  ìƒìœ„ 5ê°œ ì¶”ì¶œ
        word_counts = Counter(all_tokens)
        top_keywords = [word for word, count in word_counts.most_common(5)]
        topic_keywords[topic_num] = top_keywords       # í‚¤ì›Œë“œ ì €ì¥
        topic_counts[topic_num] = len(topic_docs)      # ë¬¸ì„œ ìˆ˜ ì €ì¥

    print(f"âœ… Extracted keywords for {len(topic_keywords)} topics")
    
    # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ Step 4: Saving results...")

    # í† í”½ì´ í• ë‹¹ëœ ë¦¬ë·° ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥
    output_csv = os.path.join(output_dir, "reviews_with_topics.csv")
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')  # í•œê¸€ ê¹¨ì§ ë°©ì§€: utf-8-sig
    print(f"   âœ… Saved: {output_csv}")

    # í† í”½ë³„ ìš”ì•½ ì •ë³´ ìƒì„±
    topic_summary = []
    for topic_num in sorted(topic_keywords.keys()):
        topic_summary.append({
            'Topic': f"Topic {topic_num}",                         # í† í”½ ë²ˆí˜¸
            'Count': topic_counts[topic_num],                      # ë¬¸ì„œ ìˆ˜
            'Keywords': ', '.join(topic_keywords[topic_num]),      # ì£¼ìš” í‚¤ì›Œë“œ
            'Percentage': f"{topic_counts[topic_num] / len(df) * 100:.1f}%"  # ë¹„ìœ¨
        })

    # í† í”½ ìš”ì•½ì„ CSVë¡œ ì €ì¥
    summary_df = pd.DataFrame(topic_summary)
    summary_csv = os.path.join(output_dir, "topic_summary.csv")
    summary_df.to_csv(summary_csv, index=False, encoding='utf-8-sig')
    print(f"   âœ… Saved: {summary_csv}")

    # 5ë‹¨ê³„: ì›ë³¸ ë¦¬ë·° ë°ì´í„°ì— í† í”½ ì •ë³´ ì¶”ê°€ (í˜ë¥´ì†Œë‚˜ ìƒì„±ìš©)
    print("\nğŸ”— Step 5: Adding topic info to original reviews...")

    # ì›ë³¸ ë¦¬ë·°ì™€ processed_dataë¥¼ ë§¤ì¹­í•˜ì—¬ í† í”½ ì •ë³´ ì¶”ê°€
    # processed_dataì˜ original_textë¥¼ í‚¤ë¡œ ì‚¬ìš©
    text_to_topic = {}
    for idx, row in df.iterrows():
        text_to_topic[row['original_text']] = row['topic']

    # ì›ë³¸ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ì— í† í”½ ì •ë³´ ì¶”ê°€
    for review in reviews:
        original_text = review.get('text', review.get('raw_text', ''))
        topic = text_to_topic.get(original_text, -1)  # ë§¤ì¹­ ì•ˆë˜ë©´ -1 (ì•„ì›ƒë¼ì´ì–´)
        review['topic'] = int(topic)

    print(f"   âœ… Added topic info to {len(reviews)} reviews")

    # ìµœì¢… ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„± ë° ë°˜í™˜
    results = {
        "topics": {int(k): v for k, v in topic_keywords.items()},       # í† í”½ë³„ í‚¤ì›Œë“œ
        "topic_counts": {int(k): v for k, v in topic_counts.items()},   # í† í”½ë³„ ë¬¸ì„œ ìˆ˜
        "docs_count": len(documents),                                    # ì „ì²´ ë¬¸ì„œ ìˆ˜
        "outliers_count": len(df[df['topic'] == -1]),                   # ì•„ì›ƒë¼ì´ì–´ ìˆ˜
        "files": {
            "reviews_csv": output_csv,      # ë¦¬ë·° íŒŒì¼ ê²½ë¡œ
            "summary_csv": summary_csv      # ìš”ì•½ íŒŒì¼ ê²½ë¡œ
        },
        "summary_table": summary_df.to_dict(orient='records'),  # ìš”ì•½ í…Œì´ë¸” (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        "reviews_with_topics": reviews  # í† í”½ ì •ë³´ê°€ ì¶”ê°€ëœ ì›ë³¸ ë¦¬ë·° (í˜ë¥´ì†Œë‚˜ ìƒì„±ìš©)
    }

    return results

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì½”ë“œ
    # ì¹´í˜ ë¦¬ë·° 5ê°œë¡œ 2ê°œì˜ í† í”½ì„ ì¶”ì¶œí•˜ëŠ” ì˜ˆì œ
    sample_reviews = [
        {"raw_text": "ì»¤í”¼ê°€ ì •ë§ ë§›ìˆì–´ìš”. ë¶„ìœ„ê¸°ë„ ì¢‹ê³  ì§ì›ë“¤ì´ ì¹œì ˆí•©ë‹ˆë‹¤."},
        {"raw_text": "ê°€ê²©ì´ ì¢€ ë¹„ì‹¸ì§€ë§Œ ë§›ì€ í›Œë¥­í•´ìš”. ì¼€ì´í¬ë„ ë§›ìˆìŠµë‹ˆë‹¤."},
        {"raw_text": "ë§¤ì¥ì´ ë„“ê³  ì¾Œì í•´ì„œ ê³µë¶€í•˜ê¸° ì¢‹ì•„ìš”. ì½˜ì„¼íŠ¸ë„ ë§ì•„ìš”."},
        {"raw_text": "ì£¼ì°¨ê°€ ë¶ˆí¸í•´ìš”. í•˜ì§€ë§Œ ì»¤í”¼ ë§› ë•Œë¬¸ì— ë‹¤ì‹œ ì˜¬ ê²ƒ ê°™ì•„ìš”."},
        {"raw_text": "ì§ì›ë¶„ë“¤ì´ ë„ˆë¬´ ì¹œì ˆí•˜ì…”ì„œ ê¸°ë¶„ì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ë¼ë–¼ ì•„íŠ¸ë„ ì˜ˆë»ìš”."}
    ]
    print(run_topic_model(sample_reviews, n_topics=2))
